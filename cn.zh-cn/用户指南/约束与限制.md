# 约束与限制<a name="dgc_01_0015"></a>

## **CDM系统级限制和约束**<a name="zh-cn_topic_0108275404_section41741324807"></a>

1.  集群创建好以后不支持修改规格，如果需要使用更高规格的，需要重新创建一个集群。
2.  CDM暂不支持控制迁移数据的速度，请避免在业务高峰期执行迁移数据的任务。
3.  当前CDM集群所有集群实例规格的网卡带宽均为1Gbps，单个实例一天传输数据量的理论极限值为10TB，对传输速度有要求的情况下可以使用多个批量数据迁移实例实现。

    上述数据量为理论极限值，实际传输数据量受数据源类型、源和目的数据源读写性能、带宽等多方面因素制约，实测最大可达到约8TB每天（大文件迁移到OBS场景）。推荐用户在正式迁移前先用小数据量实测进行速度摸底。

4.  迁移文件或对象时支持文件级增量迁移（通过配置跳过重复文件实现），但不支持断点续传。

    例如要迁移3个文件，第2个文件迁移到一半时由于网络原因失败，再次启动迁移任务时，会跳过第1个文件，从第2个文件开始重新传，但不能从第2个文件失败的位置重新传。

5.  文件迁移时，单个任务支持千万数量的文件，如果待迁移目录下文件过多，建议拆分到不同目录并创建多个任务。
6.  CDM单个实例同一时刻执行的任务数按大、中、小三种规格，分别为30、20、10。等待执行（Pending状态）的作业队列数量分别为10000、5000、2000。

    这里的一个作业在数据库迁移场景下等价于迁移一张表，在文件迁移场景下一个作业可以迁移多个文件。

7.  用户在CDM上配置的连接和作业支持导出到本地保存，考虑到密码的安全性，CDM不会将对应数据源的连接密码导出。因此在将作业配置重新导入到CDM前，需要手工编辑导出的JSON文件补充密码。
8.  不支持集群自动升级到新版本，需要用户通过作业的导出和导入功能，实现升级到新版本。
9.  在无OBS的场景下，CDM系统不会自动备份用户的作业配置，需要用户通过作业的导出功能进行备份。
10. 如果配置了VPC对等连接，可能会出现对端VPC子网与CDM管理网重叠，从而无法访问对端VPC中数据源的情况。推荐使用公网做跨VPC数据迁移，或联系管理员在CDM后台为VPC对等连接添加特定路由。
11. CDM迁移，当目的端为DWS和NewSQL的时候，不支持将源端的主键和唯一索引等约束一起迁移过去。
12. CDM迁移作业时，需确保两个集群版本的JSON文件格式保持一致，才可以从将源集群的作业导入到目标集群。

## **数据库迁移通用限制和约束**<a name="zh-cn_topic_0108275404_section18141628181814"></a>

1.  CDM以批量迁移为主，仅支持有限的数据库增量迁移，不支持数据库实时增量迁移，推荐使用数据复制服务（DRS）来实现数据库增量迁移到RDS。
2.  CDM支持的数据库整库迁移，仅支持数据表迁移，不支持存储过程、触发器、函数、视图等数据库对象迁移。

    CDM仅适用于一次性将数据库迁移到云上的场景，包括同构数据库迁移和异构数据库迁移，不适合数据同步场景，比如容灾、实时同步。

3.  CDM迁移数据库整库或数据表失败时，已经导入到目标表中的数据不会自动回滚，对于需要事务模式迁移的用户，可以配置“先导入到阶段表”参数，实现迁移失败时数据回滚。

    极端情况下，可能存在创建的阶段表或临时表无法自动删除，也需要用户手工清理（阶段表的表名以“\_cdm\_stage“结尾，例如：cdmtet\_cdm\_stage）。

4.  CDM访问用户本地数据中心数据源时（例如本地自建的MySQL数据库），需要用户的数据源可支持Internet公网访问，并为CDM集群实例绑定弹性IP。这种方式下安全实践是：本地数据源通过防火墙或安全策略仅允许CDM弹性IP访问。
5.  仅支持常用的数据类型，字符串、数字、日期，对象类型有限支持，如果对象过大会出现无法迁移的问题。
6.  仅支持数据库字符集为GBK和UTF-8。
7.  字段名不可使用&和%。

## **关系数据库迁移权限配置**<a name="zh-cn_topic_0108275404_section77841851112219"></a>

常见关系数据库迁移需要的最小权限级：

-   MySQL：INFORMATION\_SCHEMA库的读权限，以及对数据表的读权限；
-   Oracle：需要该用户有resource角色，并在tablespace下有数据表的select权限；
-   达梦：具有该schema下select any table的权限；
-   DWS：需要表的schema usage权限和数据表的查询权限。
-   SQL Server:用户需要有sysadmin权限
-   PostgreSQL:角色拥有数据库下schema下表的select权限

## **FusionInsight HD和Apache Hadoop数据源约束**<a name="zh-cn_topic_0108275404_section1443661143314"></a>

FusionInsight HD和Apache Hadoop数据源在用户本地数据中心部署时，由于读写Hadoop文件需要访问集群的所有节点，需要为每个节点都放通网络访问。

推荐使用[云专线服务](https://www.huaweicloud.com/zh-cn/product/dc.html)，解决网络访问的同时，还可以提升迁移速度。

## **数据仓库服务\(DWS\)和FusionInsight LibrA数据源约束**<a name="zh-cn_topic_0108275404_section10533131115352"></a>

1.  DWS主键或表只有一个字段时，要求字段类型必须是如下常用的字符串、数值、日期类型。从其他数据库迁移到DWS时，如果选择自动建表，主键必须为以下类型，未设置主键的情况下至少要有一个字段是以下类型，否则会无法创建表导致CDM作业失败。
    -   INTEGER TYPES：TINYINT，SMALLINT，INT，BIGINT，NUMERIC/DECIMAL
    -   CHARACTER TYPES：CHAR，BPCHAR，VARCHAR，VARCHAR2，NVARCHAR2，TEXT
    -   DATA/TIME TYPES：DATE，TIME，TIMETZ，TIMESTAMP，TIMESTAMPTZ，INTERVAL，SMALLDATETIME

2.  DWS字符类型字段认为空字符串（''）是空值，有非空约束的字段无法插入空字符串（''），这点与MySQL行为不一致，MySQL不认为空字符串（''）是空值。从MySQL迁移到DWS时，可能会因为上述原因导致迁移失败。
3.  使用GDS模式快速导入数据到DWS时，需要配置相关安全组或防火墙策略，允许DWS/LibrA的数据节点访问CDM IP地址的25000端口。
4.  使用GDS模式导入数据到DWS时，CDM会自动创建外表（foreign table）用于数据导入，表名以UUID结尾（例如：cdmtest\_aecf3f8n0z73dsl72d0d1dk4lcir8cd），作业失败正常会自动删除，极端情况下可能需要用户手工清理。

## **对象存储服务（OBS）数据源约束**<a name="zh-cn_topic_0108275404_section131751236114113"></a>

1.  迁移文件时系统会自动并发，任务配置中的“抽取并发数“无效。
2.  不支持断点续传。CDM传文件失败会产生OBS碎片，需要用户到OBS控制台清理碎片文件避免空间占用。
3.  不支持对象多版本的迁移。
4.  增量迁移时，单个作业的源端目录下的文件数量或对象数量，根据CDM集群规格分别有如下限制：大规格集群30万、中规格集群20万、小规格集群10万。

    如果单目录下文件或对象数量超过限制，需要按照子目录来拆分成多个迁移作业。


## **Oracle数据源约束**<a name="zh-cn_topic_0108275404_section15854191754519"></a>

不支持Oracle实时增量数据同步。

## **分布式缓存服务（DCS）和Redis数据源约束**<a name="zh-cn_topic_0108275404_section66348560505"></a>

1.  由于分布式缓存服务（DCS）限制了获取所有Key的命令，CDM无法支持DCS作为源端，但可以作为迁移目的端，第三方云的Redis服务也无法支持作为源端。如果是用户在本地数据中心或ECS上自行搭建的Redis支持作为源端或目的端。
2.  仅支持Hash和String两种数据格式。

## **文档数据库服务（DDS）和MongoDB数据源约束**<a name="zh-cn_topic_0108275404_section15580101719556"></a>

从MongoDB、DDS迁移到关系型数据库时，CDM会读取集合的首行数据作为字段列表样例，如果首行数据未包含该集合的所有字段，用户需要自己手工添加字段。

## **云搜索服务和Elasticsearch数据源约束**<a name="zh-cn_topic_0108275404_section025745712566"></a>

1.  CDM支持自动创建索引和类型，索引和类型名称只能全部小写，不能有大写。
2.  索引下的字段类型创建后不能修改，只能创建新字段。

    如果一定要修改字段类型，需要创建新索引或到Kibana上用Elasticsearch命令删除当前索引重新创建（数据也会删除）。

3.  CDM自动创建的索引，字段类型为date时，要求数据格式为“yyyy-MM-dd HH:mm:ss.SSS Z“，即“2018-08-08 08:08:08.888 +08:00“。

    迁移数据到云搜索服务时如果date字段的原始数据不满足格式要求，可以通过CDM的[字段转换](https://support.huaweicloud.com/bestpractice-dgc/dgc_05_0012.html)转换为上述格式。


## **数据接入服务（DIS）和Kafka数据源约束**<a name="zh-cn_topic_0108275404_section797519552109"></a>

1.  消息体中的数据是一条类似CSV格式的记录，可以支持多种分隔符。不支持二进制格式或其他格式的消息内容解析。
2.  设置为长久运行的任务，如果DIS系统发生中断，任务也会失败结束。

## **表格存储服务（CloudTable）和HBase数据源约束**<a name="zh-cn_topic_0108275404_section19323131416133"></a>

1.  CloudTable或HBase作为源端时，CDM会读取表的首行数据作为字段列表样例，如果首行数据未包含该表的所有字段，用户需要自己手工添加字段。
2.  由于HBase的无Schema技术特点，CDM无法获知数据类型，如果数据内容是使用二进制格式存储的，CDM会无法解析。

## **Hive数据源约束**<a name="zh-cn_topic_0108275404_section1557962192010"></a>

Hive作为迁移的目的时，如果存储格式为Textfile，在Hive创建表的语句中需要显式指定分隔符。例如：

```
CREATE TABLE csv_tbl(
smallint_value smallint,
tinyint_value tinyint,
int_value int,
bigint_value bigint,
float_value float,
double_value double,
decimal_value decimal(9, 7),
timestmamp_value timestamp,
date_value date,
varchar_value varchar(100),
string_value string,
char_value char(20),
boolean_value boolean,
binary_value binary,
varchar_null varchar(100),
string_null string,
char_null char(20),
int_null int
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
"separatorChar" = "\t",
"quoteChar"     = "'",
"escapeChar"    = "\\"
)
STORED AS TEXTFILE;
```

